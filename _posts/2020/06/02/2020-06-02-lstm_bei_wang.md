---
title: "LSTM 备忘"
date: 2020-06-02 23:39:00 +0800
last_modified_at: 2020-06-02 23:40:02 +0800
math: true
render_with_liquid: false
categories: ["机器学习"]
tags: ["机器学习"]
description: "长短期记忆网络（Long Short-Term Memory Network，LSTM）是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题。通过引入一个新的内部状态和门控机制来实现长短期记忆的功能。"
---

## LSTM 备忘

> 本文地址：[https://blog.lucien.ink/archives/504][this]
> 摘自邱锡鹏老师《神经网络与深度学习》的 6.6.1 节。

**长短期记忆网络**（Long Short-Term Memory Network，LSTM）[Gers et al., 2000; Hochreiter et al., 1997] 是循环神经网络的一个变体，可以有效地解决**简单循环神经网络**的梯度爆炸或消失问题。

### 1. 新的内部状态

LSTM 网络引入一个新的内部状态（internal state） $c_t \in \mathbb{ R } ^ D$ 专门进行线性的循环信息传递，同时（非线性地）输出信息给隐藏层的外部状态 $h_t \in \mathbb{ R } ^ D$。内部状态 $c_t$ 通过下面公式计算：

$$c_t = f_t \odot c_{ t - 1 } + i_t \odot \tilde{ c }_t \tag{6.51}$$$$h_t = o_t \odot \tanh(c_t) \tag{6.52}$$

其中 $f_t \in [0,1] ^ D$、$i_t \in [0, 1] ^ D$ 和 $o_t \in [0, 1] ^ D$ 为三个门（gate）来控制信息传递的路径；$\odot$ 为向量元素乘积；$c_{ t - 1 }$ 为上一时刻的记忆单元；$\tilde { c }_t \in \mathbb { R } ^ D$ 是通过非线性函数得到的**候选状态**：

$$\tilde { c }_t = \tanh(W_c x_t + U_c h_{ t - 1 } + b_c) \tag{6.53}$$

在每个时刻 $t$，LSTM 网络的内部状态 $c_t$ 记录了到当前时刻为止的历史信息。

### 2. 门控机制

在数字电路中，**门**（gate）为一个二值变量 ${0, 1}$，$0$ 代表关闭状态，不许任何信息通过；$1$ 代表开放状态，允许所有信息通过。

LSTM 网络引入**门控机制**（Gating Mechanism）来控制信息传递的路径。公式（6.51）和公式（6.52）中三个“门”分别为**输入门** $i_t$、**遗忘门** $f_t$ 和**输出门** $o_t$。这三个门的作用为：

1. 遗忘门 $f_t$ 控制上一时刻的内部状态 $c_{ t - 1 }$ 需要遗忘多少信息。
2. 输入门 $i_t$ 控制当前时刻的候选状态 $\tilde{ c }_t$ 有多少信息需要保存。
3. 输出门 $o_t$ 控制当前时刻的内部状态 $c_t$ 有多少信息需要输出给外部状态 $h_t$。

当 $f = 0,i = 1$ 时，记忆单元将历史信息清空，并将候选状态向量 $\tilde{ c }_t$ 写入。但此时记忆单元 $c_t$ 依然和上一时刻的历史信息相关。当 $f_t = 1, i_t = 0$ 时，记忆单元将复制上一时刻的内容，不写入新的信息。

LSTM 网络中的“门”是一种“软”门，取值在 $(0, 1)$ 之间，表示以一定的比例允许信息通过。三个门的计算方式为：

$$i_t = \sigma(W_i x_t + U_i h_{ t - 1 } + b_i) \tag{6.54}$$$$f_t = \sigma(W_f x_t + U_f h_{ t - 1 } + b_f) \tag{6.55}$$$$o_t = \sigma(W_o x_t + U_o h_{ t - 1 } + b_o) \tag{6.56}$$

其中 $\sigma(\cdot)$ 为 Logistic 函数，其输出区间为 $(0, 1)$，$x_t$ 为当前时刻的输入，$h_{ t - 1 }$ 为上一时刻的外部状态。

### 3. 记忆

循环神经网络中的隐状态 $h$ 存储了历史信息，可以看作一种**记忆**（Memory）。在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作一种**短期记忆**（Short-Term Memory）。在神经网络中，**长期记忆**（Long-Term Memory）可以看作网络参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆。而在 LSTM 网络中，记忆单元 $c$ 可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔。记忆单元 $c$ 中保存信息的生命周期要长于短期记忆 $h$，但又远远短于长期记忆，因此称为**长短期记忆**（Long Short-Term Memory）。

> 一般在深度网络参数学习时，参数初始化的值一般都比较小。但是在训 练 LSTM 网络时，过小的值会使得遗忘门的值比较小。这意味着前一时刻的信息大部分都丢失了，这样网络很难捕捉到长距离的依赖信息。并且相邻时间间隔的梯度会非常小，这会导致梯度弥散问题。因此遗忘的参数初始值一般都设得比较大，其偏置向量 $b_f$ 设为 $1$ 或 $2$。

[this]: https://blog.lucien.ink/archives/504/